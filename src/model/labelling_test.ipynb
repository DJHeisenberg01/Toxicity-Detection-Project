{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../../data/processed/cleaned_twitch_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f673671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856427d3",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "https://medium.com/@danielafrimi/text-clustering-using-nlp-techniques-c2e6b08b6e95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0f9b3",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bf468",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding with method: Tf-Idf\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "X = vectorizer.fit_transform(df['message']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7988a1c",
   "metadata": {},
   "source": [
    "## Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding with method: Sentence transformer\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "df['encode_transforemers'] = df['message'].apply(lambda text: model.encode(text, convert_to_numpy=True).flatten())\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(et - st))\n",
    "\n",
    "X_transformers = np.vstack(df['encode_transforemers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73003bb",
   "metadata": {},
   "source": [
    "## BERT - [CLS] token for sentence context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_cls_sentence(sentence):\n",
    "    # Tokenize input sentence and convert to tensor\n",
    "    input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True, max_length=512)])\n",
    "\n",
    "    # Pass input through BERT model and extract embeddings for [CLS] token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        cls_embedding = outputs[0][:, 0, :]\n",
    "    \n",
    "    return cls_embedding.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e869320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding with method: BERT-[CLS]\")\n",
    "\n",
    "st = time.time()\n",
    "df['cls_bert'] = df['message'].apply(lambda sentence: get_cls_sentence(sentence))\n",
    "et = time.time()\n",
    "\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(et - st))\n",
    "\n",
    "X_cls_bert = np.vstack(df['cls_bert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b81ce4",
   "metadata": {},
   "source": [
    "## Salvataggio dei risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/processed/twitch_messages_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a4194",
   "metadata": {},
   "source": [
    "## Clustering e visualizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def eval_cluster(embedding, kmeans):\n",
    "#    y_pred = kmeans.fit_predict(embedding)\n",
    "#    \n",
    "#    # Evaluate the performance using ARI, NMI, and FMI\n",
    "#    ari = adjusted_rand_score(df[\"target\"], y_pred)\n",
    "#    nmi = normalized_mutual_info_score(df[\"target\"], y_pred)\n",
    "#    fmi = fowlkes_mallows_score(df[\"target\"], y_pred)\n",
    "#\n",
    "#    # Print Metrics scores\n",
    "#    print(\"Adjusted Rand Index (ARI): {:.3f}\".format(ari))\n",
    "#    print(\"Normalized Mutual Information (NMI): {:.3f}\".format(nmi))\n",
    "#    print(\"Fowlkes-Mallows Index (FMI): {:.3f}\".format(fmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a031559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(embedding, method):\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "    pca_vecs = pca.fit_transform(embedding)\n",
    "\n",
    "    # save our two dimensions into x0 and x1\n",
    "    x0 = pca_vecs[:, 0]\n",
    "    x1 = pca_vecs[:, 1]\n",
    "    \n",
    "    df[f'x0_{method}'] = x0 \n",
    "    df[f'x1_{method}'] = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90721578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(x0_name, x1_name, cluster_name, method):\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    plt.title(f\"KMeans clustering with {method}\", fontdict={\"fontsize\": 18})\n",
    "    plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
    "    plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "    sns.scatterplot(data=df, x=x0_name, y=x1_name, hue=cluster_name, palette=\"viridis\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clustering and visualization\")\n",
    "\n",
    "for embedding_and_method in [(X, 'tfidf'), (X_transformers, 'transformers'), ((X_cls_bert, 'Bert-CLS')) ]:\n",
    "    embedding, method = embedding_and_method[0], embedding_and_method[1]\n",
    "    \n",
    "    # initialize kmeans with 3 centroids\n",
    "    kmeans = KMeans(n_clusters=2, random_state=79872435)\n",
    "\n",
    "    # fit the model\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    # store cluster labels in a variable\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    # Assign clusters to our dataframe\n",
    "    clusters_result_name = f'cluster_{method}'\n",
    "    df[clusters_result_name] = clusters\n",
    "    \n",
    "    #eval_cluster(embedding, kmeans)\n",
    "    \n",
    "    dimension_reduction(embedding, method)\n",
    "    \n",
    "    plot_pca(f'x0_{method}', f'x1_{method}', cluster_name=clusters_result_name, method=method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
