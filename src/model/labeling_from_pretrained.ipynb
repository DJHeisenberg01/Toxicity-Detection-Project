{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello\n",
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i messaggi\n",
    "df = pd.read_csv(\"../../data/processed/cleaned_twitch_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per stimare la tossicità\n",
    "def get_toxicity_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Usa il punteggio massimo tra le 6 classi\n",
    "    score = torch.sigmoid(outputs.logits)[0].max().item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica ai messaggi\n",
    "tqdm.pandas()\n",
    "df[\"toxicity_score\"] = df[\"message\"].progress_apply(get_toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etichette: 1 = tossico, 0 = non tossico, -1 = ambiguo\n",
    "#df[\"label\"] = df[\"toxicity_score\"].apply(lambda x: 1 if x >= 0.7 else (0 if x <= 0.3 else -1))\n",
    "df[\"label\"] = df[\"toxicity_score\"].apply(lambda x: 1 if x >= 0.9 else 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"messages_with_toxicity_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(\"./data/messages_with_toxicity_labels.csv\", index=False)\n",
    "print(\"✅ Etichettatura completata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Visualizzazione del punteggio\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x=\"toxicity_score\", bins=30, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribuzione dei punteggi di tossicità\")\n",
    "plt.xlabel(\"Toxicity score\")\n",
    "plt.ylabel(\"Numero di messaggi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con CitizenLab distilbert-base-multilingual-cased-toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modello multilingua specializzato in tossicità\n",
    "MODEL_NAME = \"citizenlab/distilbert-base-multilingual-cased-toxicity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_classifier = pipeline(\"text-classification\", model=MODEL_NAME, tokenizer=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per etichettare e ottenere score\n",
    "def classify_toxicity(text):\n",
    "    result = toxicity_classifier(text[:512])[0]  # truncation manuale max input\n",
    "    label = 1 if result[\"label\"] == \"toxic\" else 0\n",
    "    score = result[\"score\"]\n",
    "    return pd.Series([label, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica con progress bar\n",
    "tqdm.pandas()\n",
    "df[[\"label\", \"toxicity_score\"]] = df[\"message\"].progress_apply(classify_toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etichette: soglie personalizzabili\n",
    "df[\"label\"] = df[\"toxicity_score\"].apply(lambda x: 1 if x >= 0.6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"messages_labeled_citizenlab.csv\", index=False)\n",
    "print(\"✅ Etichettatura completata con il modello CitizenLab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./messages_labeled_citizenlab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"toxicity_score\"], bins=70, kde=True, color=\"salmon\")\n",
    "plt.title(\"Distribuzione dei punteggi di tossicità (CitizenLab)\")\n",
    "plt.xlabel(\"Toxicity score\")\n",
    "plt.ylabel(\"Numero di messaggi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"toxicity_score > 0.99\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NotSoResidentSleeper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
